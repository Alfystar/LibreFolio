# Phase 5.1: FA Metadata & Classification + Provider Integration - Implementation Checklist

**Reference**: `specifiche_fase_5-1.md`  
**Project**: LibreFolio - Asset Metadata Management  
**Start Date**: November 19, 2025  
**Estimated Duration**: 4-6 days  
**Status**: ðŸ”´ **NOT STARTED**

---

## ðŸ“Œ High-Level Overview

**Goal**: Implement comprehensive asset metadata management with:
- Classification parameters (geographic_area, investment_type)
- Provider metadata auto-populate on assignment
- Bulk-first API pattern with partial success support
- Pydantic schema validation (NO inline models)
- Normalization utilities (pycountry, Decimal quantization)

**Key Principles**:
- âœ… Reuse existing code before creating new (schemas, services, utilities)
- âœ… Bulk-first: Primary endpoints are bulk, singles call bulk with 1 item
- âœ… Partial success: Independent operations report per-item results
- âœ… Block validation: classification_params.geographic_area is indivisible
- âœ… Pydantic v2: All schemas in `backend/app/schemas/`, FA prefix convention

---

## Phase 0: Pre-Implementation Analysis (1 day) âœ… COMPLETED

**Status**: âœ… **COMPLETED** - November 19, 2025

### 0.1 Code Audit & Reuse Identification

- [x] **Audit existing Pydantic schemas** (`backend/app/schemas/`)
  - File: Checked `assets.py`, `provider.py`, `prices.py`, `common.py`
  - Look for: Bulk request/response patterns, asset ID lists, result per-item structures
  - Document: List reusable schemas and where they're used
  - **Action**: âœ… Created `SCHEMA_REUSE_ANALYSIS.md` in dev journal
  - **Result**: Identified reusable patterns: `FAProviderAssignmentResult`, `FABulkAssignResponse`

- [x] **Audit decimal/numeric utilities** (`backend/app/utils/`)
  - File: Checked `decimal_utils.py`, `financial_math.py`
  - Look for: `parse_decimal_value()`, `truncate_to_db_precision()`, quantization logic
  - Document: Available utility functions and their signatures
  - **Action**: âœ… Documented in `SCHEMA_REUSE_ANALYSIS.md`
  - **Result**: Found 4 utility functions, `parse_decimal_value()` exists in `financial_math.py`

- [x] **Audit normalization patterns**
  - Files: Checked services (`asset_source.py`, `fx.py`) for existing normalization
  - Look for: Country mapping, currency normalization, weight distributions
  - Document: Patterns that can be extended for geographic_area
  - **Action**: âœ… Documented reusable patterns
  - **Result**: FX uppercase normalization pattern can be extended for ISO-3166-A3

- [x] **Check pycountry availability**
  - Command: `pipenv graph | grep pycountry`
  - **Result**: âŒ NOT INSTALLED - needs installation
  - **Next**: Add to Pipfile and install: `pipenv install pycountry`
  - Test import: `python -c "import pycountry; print(pycountry.countries.get(alpha_2='IT'))"`

---

## Phase 1: Database Schema Updates (1 day) âœ… COMPLETED

**Status**: âœ… **COMPLETED** - November 19, 2025

### 1.1 Asset Model Extensions

- [x] **Add classification_params field to Asset model**
  - File: âœ… `backend/app/db/models.py` (class Asset, line ~388)
  - Added after `interest_schedule`:
    ```python
    classification_params: Optional[str] = Field(default=None, sa_column=Column(Text))
    ```
  - **Result**: Column added with proper comment documentation

- [x] **Update Asset docstring**
  - Added comprehensive section documenting `classification_params` structure
  - Note: geographic_area uses ISO-3166-A3 codes, weights must sum to 1.0
  - Note: Validation via ClassificationParamsModel Pydantic schema
  - **Result**: Complete JSON structure example included

### 1.2 Direct Schema Update (Pre-Beta: No Alembic Migration)

**Note**: Since we're in pre-beta phase, we modified `001_initial.py` directly instead of creating a new migration.

- [x] **Update 001_initial.py to add classification_params column**
  - File: âœ… `backend/alembic/versions/001_initial.py`
  - Found the `op.create_table('assets', ...)` section (line ~41)
  - Added column after `interest_schedule`:
    ```python
    classification_params TEXT,
    ```
  - **Result**: Migration updated

- [x] **Recreate databases from scratch**
  - Test DB: 
    ```bash
    rm backend/data/sqlite/test_app.db
    ./dev.sh db:upgrade backend/data/sqlite/test_app.db
    ```
  - Prod DB:
    ```bash
    rm backend/data/sqlite/app.db
    ./dev.sh db:upgrade backend/data/sqlite/app.db
    ```
  - Verified column: `sqlite3 backend/data/sqlite/test_app.db "PRAGMA table_info(assets)"`
  - **Expected**: âœ… Column `classification_params` at position 8, type TEXT, nullable=0 (NULL allowed)

- [x] **Update schema validation test**
  - File: `backend/test_scripts/test_db/db_schema_validate.py`
  - Auto-detected new column (dynamic test)
  - Run: `./test_runner.py db validate`
  - **Expected**: âœ… PASS - Schema validation successful

---

## Phase 2: Pydantic Schemas (2 days) âœ… COMPLETED

**Status**: âœ… **COMPLETED** - November 19, 2025

**Note**: All schemas defined in dedicated modules following Phase 5 refactoring patterns (no inline definitions).

### 2.1 Review Existing Schemas (Reuse Analysis)

- [ ] **Document existing bulk patterns**
  - Files: `schemas/provider.py`, `schemas/prices.py`, `schemas/refresh.py`
  - Identify: `FABulkAssignRequest`, `FABulkUpsertRequest`, result structures
  - Pattern: `{ "assets": [...] }` or `{ "asset_ids": [...] }`
  - Pattern: Result = `{ "asset_id": int, "success": bool, "message": str, ... }`
  - **Action**: Create reusable base classes if patterns are identical

### 2.2 Geographic Area & Classification Schemas

- [ ] **Create geographic area utility module**
  - File: `backend/app/utils/geo_normalization.py` (NEW)
  - **Reuse**: âœ… `parse_decimal_value()` from `financial_math.py` (already exists)
  - Functions to implement:
    1. `normalize_country_to_iso3(input: str) -> str` - Use pycountry to map name/ISO2/ISO3 to ISO-3166-A3
    2. `parse_decimal_weight(value: int | float | str | Decimal) -> Decimal` - **REUSE** `parse_decimal_value()` from financial_math
    3. `validate_and_normalize_geographic_area(data: dict[str, Any]) -> dict[str, Decimal]` - Full pipeline:
       - Map all countries to ISO-3166-A3 (using pycountry âœ… installed)
       - Parse all weights to Decimal (reuse existing function)
       - Check sum tolerance (abs(sum - 1) <= 1e-6)
       - Quantize to 4 decimals (ROUND_HALF_EVEN) - pattern from `decimal_utils.py`
       - Renormalize on smallest weight if sum != 1.0
       - Return validated dict or raise ValueError with details
  - **Imports**: `pycountry` âœ…, `decimal.Decimal`, `typing`, `backend.app.utils.financial_math.parse_decimal_value`

- [x] **Create tests for geo_normalization**
  - File: âœ… `backend/test_scripts/test_utilities/test_geo_normalization.py` (240 lines)
  - Test cases:
    1. âœ… Valid ISO-3166-A3 codes (USA, GBR, ITA)
    2. âœ… ISO-2 to ISO-3 conversion (US â†’ USA, GB â†’ GBR)
    3. âœ… Country names to ISO-3 (United States â†’ USA, Italy â†’ ITA)
    4. âœ… Invalid country â†’ raises ValueError
    5. âœ… Weights as strings â†’ converts to Decimal
    6. âœ… Sum within tolerance â†’ quantizes correctly
    7. âœ… Sum out of tolerance â†’ renormalizes on smallest weight
    8. âœ… Sum too far (e.g., 0.5 total) â†’ raises ValueError
  - Run: âœ… `./test_runner.py utils geo-normalization`
  - **Result**: âœ… ALL TESTS PASSING (added to test_runner.py)

- [ ] **Create GeographicArea Pydantic model**
  - File: `backend/app/schemas/assets.py` (UPDATE)
  - Model:
    ```python
    class GeographicAreaModel(BaseModel):
        """Geographic area distribution (ISO-3166-A3, weights sum to 1.0).
        
        Example: {"USA": 0.60, "EUR": 0.30, "GBR": 0.10}
        """
        model_config = ConfigDict(extra="forbid")
        
        # Dynamic dict of ISO-3166-A3 codes to Decimal weights
        # Validation happens via root_validator
        __root__: dict[str, Decimal]
        
        @field_validator('__root__')
        @classmethod
        def validate_geographic_area(cls, v):
            from backend.app.utils.geo_normalization import validate_and_normalize_geographic_area
            return validate_and_normalize_geographic_area(v)
    ```

- [ ] **Create ClassificationParams Pydantic model**
  - File: `backend/app/schemas/assets.py` (UPDATE)
  - Model:
    ```python
    class ClassificationParamsModel(BaseModel):
        """Asset classification metadata.
        
        All fields optional (partial updates supported).
        geographic_area is indivisible block (full replace on update).
        """
        model_config = ConfigDict(extra="forbid")
        
        investment_type: Optional[str] = Field(None, description="Investment type (stock, etf, bond, etc.)")
        short_description: Optional[str] = Field(None, max_length=500, description="Brief description")
        geographic_area: Optional[dict[str, Decimal]] = Field(None, description="Geographic distribution (ISO-3166-A3, sum=1.0)")
        sector: Optional[str] = Field(None, max_length=100, description="Sector classification")
        
        @field_validator('geographic_area')
        @classmethod
        def validate_geo_area(cls, v):
            if v is None:
                return None
            from backend.app.utils.geo_normalization import validate_and_normalize_geographic_area
            return validate_and_normalize_geographic_area(v)
    ```

### 2.3 Metadata PATCH Request/Response Schemas

- [x] **Create PATCH metadata request schema**
  - File: âœ… `backend/app/schemas/assets.py` (UPDATED)
  - Model: âœ… `PatchAssetMetadataRequest` created
  - Fields: investment_type, short_description, geographic_area, sector (all Optional)
  - PATCH semantics: absent = ignore, null = clear, value = update
  - geographic_area: full block replace (no partial merge)
  - ConfigDict: extra="forbid"

- [x] **Create metadata response schemas**
  - File: âœ… `backend/app/schemas/assets.py` (UPDATED)
  - Models created:
    - âœ… `AssetMetadataResponse` - asset with metadata (asset_id, display_name, identifier, currency, classification_params)
    - âœ… `MetadataChangeDetail` - single field change (field, old_value, new_value)
    - âœ… `MetadataRefreshResult` - per-asset refresh result (asset_id, success, message, changes, warnings)
  - Follows FA pattern: { asset_id, success, message, ... }
        
    class MetadataRefreshResult(BaseModel):
        """Result of metadata refresh for single asset."""
        asset_id: int
        success: bool
        message: str
        changes: Optional[list[MetadataChangeDetail]] = None
        warnings: Optional[list[str]] = None
    ```

### 2.4 Bulk Read Schemas

- [x] **Create bulk asset read request**
  - File: `backend/app/schemas/assets.py` (UPDATE)
  - Check if `BulkAssetIdsRequest` already exists in provider.py or prices.py
  - If exists: **REUSE**, add to assets.py imports
  - If not: Create:
    ```python
    class BulkAssetReadRequest(BaseModel):
        """Request to read multiple assets by IDs."""
        model_config = ConfigDict(extra="forbid")
        
        asset_ids: list[int] = Field(..., min_length=1, max_length=1000, description="Asset IDs to fetch")
    ```

- [x] **Create bulk metadata refresh request/response**
  - File: `backend/app/schemas/assets.py` (UPDATE)
  - Models:
    ```python
    class BulkMetadataRefreshRequest(BaseModel):
        """Bulk metadata refresh request."""
        model_config = ConfigDict(extra="forbid")
        
        asset_ids: list[int] = Field(..., min_length=1, max_length=100, description="Assets to refresh")
        
    class BulkMetadataRefreshResponse(BaseModel):
        """Bulk metadata refresh response (partial success)."""
        results: list[MetadataRefreshResult]
        success_count: int
        failed_count: int
    ```

### 2.5 Provider Metadata Fetch Schema

- [x] **Define provider metadata return structure**
  - File: Add docstring to `AssetSourceProvider` abstract class
  - New method signature (to be implemented by providers):
    ```python
    async def fetch_asset_metadata(
        self,
        identifier: str,
        provider_params: dict | None = None
    ) -> dict | None:
        """Fetch metadata from provider (optional, not all providers support this).
        
        Returns:
            dict with keys: investment_type, short_description, geographic_area, sector
            Or None if provider doesn't support metadata or asset not found
            
        Note: 
        - Plugin returns RAW data (no normalization side effects)
        - Core handles normalization/validation/persistence
        """
        return None  # Default: no metadata support
    ```

### 2.6 Schema Export Updates

- [x] **Update schemas/__init__.py**
  - File: âœ… `backend/app/schemas/__init__.py` (UPDATED)
  - Added 8 new imports from assets.py:
    - âœ… ClassificationParamsModel
    - âœ… PatchAssetMetadataRequest
    - âœ… AssetMetadataResponse
    - âœ… MetadataChangeDetail
    - âœ… MetadataRefreshResult
    - âœ… BulkAssetReadRequest
    - âœ… BulkMetadataRefreshRequest
    - âœ… BulkMetadataRefreshResponse
  - Updated `__all__` list: 32 â†’ 40 exports (+25%)
  - **Verification**: âœ… `from backend.app.schemas import ClassificationParamsModel` works

---

## Phase 3: Core Service Layer (2 days) âœ… COMPLETED

**Status**: âœ… **COMPLETED** - November 19, 2025  
**Duration**: ~50 minutes total (3.1: 15min, 3.2: 20min, 3.3: 15min)

### 3.1 Metadata Normalization Service âœ… COMPLETED

- [x] **Create metadata service module**
  - File: âœ… `backend/app/services/asset_metadata.py` (264 lines)
  - Class: âœ… `AssetMetadataService` (static methods)
  - **Result**: Service module created and tested

- [x] **Implement parse_classification_params()**
  - Signature: âœ… `@staticmethod def parse_classification_params(json_str: Optional[str]) -> ClassificationParamsModel | None`
  - Logic:
    1. âœ… If `json_str` is None or empty â†’ return None
    2. âœ… Parse JSON â†’ dict
    3. âœ… Validate with `ClassificationParamsModel(**data)`
    4. âœ… Return validated model
  - **Reuse**: âœ… Import geo_normalization utilities (in Pydantic validator)
  - **Test**: âœ… Passed - `âœ… Parse: stock, geo_area keys: ['USA', 'ITA']`

- [x] **Implement serialize_classification_params()**
  - Signature: âœ… `@staticmethod def serialize_classification_params(model: ClassificationParamsModel | None) -> str | None`
  - Logic:
    1. âœ… If model is None â†’ return None
    2. âœ… Convert to dict: `model.model_dump(exclude_none=True)`
    3. âœ… Serialize to JSON: `json.dumps(data)`
    4. âœ… Return JSON string
  - **Test**: âœ… Passed - `âœ… Serialize: 77 chars`

- [x] **Implement compute_metadata_diff()**
  - Signature: âœ… `@staticmethod def compute_metadata_diff(old, new) -> list[MetadataChangeDetail]`
  - Logic:
    1. âœ… Compare old vs new field by field
    2. âœ… Track changes: `{ field, old_value, new_value }`
    3. âœ… Return list of changes
  - Special handling: âœ… geographic_area (dict comparison)
  - **Test**: âœ… Passed - `âœ… Diff: 2 changes detected`

- [x] **Implement apply_partial_update()**
  - Signature: âœ… `@staticmethod def apply_partial_update(current, patch) -> ClassificationParamsModel`
  - Logic (PATCH semantics):
    1. âœ… Start with current params (or empty if None)
    2. âœ… For each field in patch:
       - **Not present** in patch dict (absent key) â†’ **ignore**, keep current
       - **null in JSON** (None in Python) â†’ **clear** field
       - **Value present** â†’ **update** field
    3. âœ… Special: geographic_area is **full replace** (no merge)
    4. âœ… Validate result with `ClassificationParamsModel`
    5. âœ… Return updated model
  - **Note**: âœ… Use `patch.model_dump(exclude_unset=True)` to distinguish absent vs null
  - **Test**: âœ… Passed - `âœ… PATCH: sector cleared, investment_type=stock`

- [x] **BONUS: Implement merge_provider_metadata()**
  - Signature: âœ… `@staticmethod def merge_provider_metadata(current, provider_data) -> ClassificationParamsModel`
  - Logic: Provider data merges with current (provider takes precedence)
  - **Status**: âœ… Implemented for Phase 4 provider integration

### 3.2 Provider Integration in AssetSourceManager âœ… COMPLETED

- [x] **Update bulk_assign_providers() for auto-populate**
  - File: âœ… `backend/app/services/asset_source.py` (UPDATED, line ~274)
  - After each successful assignment:
    1. âœ… Call `provider.fetch_asset_metadata(identifier, provider_params)`
    2. âœ… If metadata returned:
       - âœ… Normalize via `AssetMetadataService`
       - âœ… Compute diff
       - âœ… Persist to `asset.classification_params`
       - âœ… Add `metadata_changes` to result dict
    3. âœ… If metadata is None or provider doesn't support â†’ skip (no error)
  - **Logging**: âœ… Uses structlog to log metadata auto-populate events
  - **Reuse**: âœ… Import `AssetMetadataService` methods
  - **Error Handling**: âœ… Non-blocking (logs warning, doesn't fail assignment)

- [x] **Create refresh_asset_metadata() method**
  - File: âœ… `backend/app/services/asset_source.py` (UPDATED, ~165 lines)
  - Method: âœ… `@staticmethod async def refresh_asset_metadata(asset_id: int, session: AsyncSession) -> dict`
  - Logic:
    1. âœ… Load asset + provider assignment
    2. âœ… If no provider â†’ return `{ success: False, message: "No provider assigned" }`
    3. âœ… Get provider instance
    4. âœ… Call `fetch_asset_metadata()`
    5. âœ… If None â†’ return `{ success: False, message: "Provider doesn't support metadata" }`
    6. âœ… Normalize, compute diff, persist
    7. âœ… Return `{ success: True, message: "...", changes: [...] }`
  - **Result**: Full implementation with comprehensive error handling

- [x] **Create bulk_refresh_metadata() method**
  - File: âœ… `backend/app/services/asset_source.py` (UPDATED, ~40 lines)
  - Method: âœ… `@staticmethod async def bulk_refresh_metadata(asset_ids: list[int], session: AsyncSession) -> dict`
  - Logic:
    1. âœ… For each asset_id: call `refresh_asset_metadata()`
    2. âœ… Collect results (partial success)
    3. âœ… Return `{ results: [...], success_count: N, failed_count: M }`
  - **Optimization**: âœ… Comment added noting parallelization possibility with `asyncio.gather()`
  - **Result**: Full bulk implementation with success/failure counts

### 3.3 PATCH Metadata Service Method âœ… COMPLETED

- [x] **Create update_asset_metadata() method**
  - File: âœ… `backend/app/services/asset_metadata.py` (UPDATED, +100 lines)
  - Method: âœ… `@staticmethod async def update_asset_metadata(asset_id: int, patch: PatchAssetMetadataRequest, session: AsyncSession) -> AssetMetadataResponse`
  - Logic:
    1. âœ… Load asset from DB (`select(Asset).where(Asset.id == asset_id)`)
    2. âœ… Parse current `classification_params` via `parse_classification_params()`
    3. âœ… Apply patch via `apply_partial_update()`
    4. âœ… Validate result (geographic_area block validation in Pydantic)
    5. âœ… Serialize back to JSON via `serialize_classification_params()`
    6. âœ… Update asset.classification_params
    7. âœ… Commit transaction (`await session.commit()`)
    8. âœ… Refresh asset (`await session.refresh(asset)`)
    9. âœ… Return `AssetMetadataResponse`
  - Error handling: âœ… ValueError if asset not found or validation fails (422 in API layer)
  - **Result**: Full async PATCH implementation ready for API endpoints

---

## Phase 4: Provider Plugin Updates (1 day) âœ… COMPLETED

**Status**: âœ… **COMPLETED** - November 19, 2025  
**Duration**: ~15 minutes

**Note**: Metadata fetch is OPTIONAL. Not all providers need to implement it.

### 4.1 Update Existing Providers (Optional Metadata Support) âœ…

- [x] **YahooFinance: Add fetch_asset_metadata()**
  - File: âœ… `backend/app/services/asset_source_providers/yahoo_finance.py` (UPDATED, +95 lines)
  - Implementation: Full metadata extraction from yfinance ticker.info
  - Fields returned:
    - âœ… `investment_type`: Mapped from quoteType (equityâ†’stock, etfâ†’etf, etc.)
    - âœ… `short_description`: From longBusinessSummary (truncated to 500 chars) or names
    - âœ… `sector`: From sector field
    - âŒ `geographic_area`: Not available from Yahoo Finance (returns None)
  - Error handling: Returns None on any exception, logs warning
  - **Result**: Full implementation ready for auto-populate

- [x] **CSS Scraper: Add fetch_asset_metadata() stub**
  - File: âœ… `backend/app/services/asset_source_providers/css_scraper.py` (UPDATED, +20 lines)
  - Implementation: Returns None (not supported for manual/CSV providers)
  - Rationale: User enters metadata manually for custom assets
  - **Result**: Stub added, no breaking changes

- [x] **ScheduledInvestment: Add fetch_asset_metadata() stub**
  - File: âœ… `backend/app/services/asset_source_providers/scheduled_investment.py` (UPDATED, +20 lines)
  - Implementation: Returns None (synthetic provider, no external metadata)
  - Rationale: Calculated values from interest schedule, no external source
  - **Result**: Stub added, no breaking changes

- [x] **MockProv: Add fetch_asset_metadata() for testing**
  - File: âœ… `backend/app/services/asset_source_providers/mockprov.py` (UPDATED, +30 lines)
  - Implementation: Returns predictable mock data for testing
  - Mock data returned:
    ```python
    {
        "investment_type": "stock",
        "short_description": "Mock test asset {identifier} - used for testing metadata features",
        "geographic_area": {"USA": "0.6", "ITA": "0.4"},  # Tests string parsing
        "sector": "Technology"
    }
    ```
  - **Result**: Full mock implementation for testing auto-populate flows
  - Implementation:
    ```python
    async def fetch_asset_metadata(self, identifier: str, provider_params: dict | None = None) -> dict | None:
        """Fetch metadata from Yahoo Finance (sector, description, etc.)."""
        try:
            ticker = yf.Ticker(identifier)
            info = ticker.info
            
            return {
                "investment_type": "stock",  # Could map from info.get('quoteType')
                "short_description": info.get('longBusinessSummary', '')[:500],
                "sector": info.get('sector'),
                # geographic_area: Not available from yfinance
            }
        except Exception as e:
            logger.warning(f"Could not fetch metadata for {identifier}: {e}")
            return None
    ```

- [x] **CSS Scraper: Add fetch_asset_metadata() stub**
  - File: `backend/app/services/asset_source_providers/css_scraper.py`
  - Return None (not supported for manual/CSV providers)

- [x] **ScheduledInvestment: Add fetch_asset_metadata() stub**
  - File: `backend/app/services/asset_source_providers/scheduled_investment.py`
  - Return None (synthetic provider, no external metadata)

- [x] **MockProv: Add fetch_asset_metadata() for testing**
  - File: `backend/app/services/asset_source_providers/mockprov.py`
  - Return mock data for test cases:
    ```python
    {
        "investment_type": "stock",
        "short_description": "Mock test asset",
        "geographic_area": {"USA": "0.6", "ITA": "0.4"},  # Test string parsing
        "sector": "Technology"
    }
    ```

---

## Phase 5: API Endpoints Implementation (2 days)

### 5.1 Metadata Management Endpoints

- [x] **PATCH /api/v1/assets/metadata (NEW BULK)**
  - File: `backend/app/api/v1/assets.py` (UPDATE)
  - Endpoint:
    ```python
    from fastapi import Depends, HTTPException
    from sqlalchemy.ext.asyncio import AsyncSession
    from backend.app.services.asset_metadata import AssetMetadataService
    from backend.app.schemas.assets import (
        BulkPatchAssetMetadataRequest,
        MetadataRefreshResult,
    )

    @router.patch("/metadata", response_model=list[MetadataRefreshResult])
    async def update_assets_metadata_bulk(
        request: BulkPatchAssetMetadataRequest,
        session: AsyncSession = Depends(get_session_generator)
    ):
        """Bulk partial update of asset metadata (FA pattern)."""
        try:
            results = []
            for item in request.assets:
                try:
                    res = await AssetMetadataService.update_asset_metadata(item.asset_id, item.patch, session)
                    results.append({
                        "asset_id": item.asset_id,
                        "success": True,
                        "message": "updated",
                        "changes": getattr(res, "changes", None)
                    })
                except ValueError as e:
                    results.append({
                        "asset_id": item.asset_id,
                        "success": False,
                        "message": str(e)
                    })
                except Exception as e:
                    logger.error(f"Error updating metadata for asset {item.asset_id}: {e}")
                    results.append({
                        "asset_id": item.asset_id,
                        "success": False,
                        "message": "internal error"
                    })
            return results
        except Exception as e:
            logger.error(f"Error in bulk metadata update: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    ```
  - Notes:
    - Request schema: `BulkPatchAssetMetadataRequest` / `PatchAssetMetadataItem` (asset_id + patch payload)
    - Follows FA per-item result pattern (`MetadataRefreshResult` with optional `changes`)
    - Returns 422 when payload fails validation (list required, per-item patch validation enforced)
    - Makes sure doc blocks import `AsyncSession`, `Depends`, `BulkPatchAssetMetadataRequest`, `MetadataRefreshResult`, and `AssetMetadataService` to avoid IDE irons

- [x] **POST /api/v1/assets (bulk read) (MODIFY EXISTING or NEW)**
  - File: `backend/app/api/v1/assets.py` (UPDATE)
  - Check if `GET /api/v1/assets/{asset_id}` exists
  - Create new bulk endpoint:
    ```python
    @router.post("", response_model=list[AssetMetadataResponse])  # Note: POST to /api/v1/assets (no trailing slash)
    async def read_assets_bulk(
        request: BulkAssetReadRequest,
        session: AsyncSession = Depends(get_session_generator)
    ):
        """Read multiple assets with metadata (bulk-first pattern)."""
        try:
            assets = await session.execute(
                select(Asset).where(Asset.id.in_(request.asset_ids))
            )
            assets = assets.scalars().all()
            
            return [
                AssetMetadataResponse(
                    asset_id=asset.id,
                    display_name=asset.display_name,
                    identifier=asset.identifier,
                    currency=asset.currency,
                    classification_params=AssetMetadataService.parse_classification_params(
                        asset.classification_params
                    )
                )
                for asset in assets
            ]
        except Exception as e:
            logger.error(f"Error reading assets bulk: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    ```

- [x] **POST /api/v1/assets/{asset_id}/metadata/refresh (NEW)**
  - File: `backend/app/api/v1/assets.py` (UPDATE)
  - Endpoint:
    ```python
    @router.post("/{asset_id}/metadata/refresh", response_model=MetadataRefreshResult)
    async def refresh_asset_metadata_single(
        asset_id: int,
        session: AsyncSession = Depends(get_session_generator)
    ):
        """Force refresh metadata from provider (single asset)."""
        try:
            result = await AssetSourceManager.refresh_asset_metadata(asset_id, session)
            return MetadataRefreshResult(**result)
        except Exception as e:
            logger.error(f"Error refreshing metadata for asset {asset_id}: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    ```

- [x] **POST /api/v1/assets/metadata/refresh/bulk (NEW, optional but recommended)**
  - File: `backend/app/api/v1/assets.py` (UPDATE)
  - Endpoint:
    ```python
    @router.post("/metadata/refresh/bulk", response_model=BulkMetadataRefreshResponse)
    async def refresh_asset_metadata_bulk(
        request: BulkMetadataRefreshRequest,
        session: AsyncSession = Depends(get_session_generator)
    ):
        """Bulk metadata refresh (PRIMARY bulk endpoint, partial success)."""
        try:
            result = await AssetSourceManager.bulk_refresh_metadata(request.asset_ids, session)
            return BulkMetadataRefreshResponse(**result)
        except Exception as e:
            logger.error(f"Error in bulk metadata refresh: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    ```

### 5.2 Provider Assignment Bulk Read Endpoint

- [ ] **POST /api/v1/assets/scraper (NEW - bulk read provider assignments)**
  - File: `backend/app/api/v1/assets.py` (UPDATE)
  - Endpoint:
    ```python
    @router.post("/scraper", response_model=list[dict])
    async def read_provider_assignments_bulk(
        request: BulkAssetReadRequest,
        session: AsyncSession = Depends(get_session_generator)
    ):
        """Read provider assignments for multiple assets."""
        try:
            assignments = await session.execute(
                select(AssetProviderAssignment).where(
                    AssetProviderAssignment.asset_id.in_(request.asset_ids)
                )
            )
            assignments = assignments.scalars().all()
            
            return [
                {
                    "asset_id": a.asset_id,
                    "provider_code": a.provider_code,
                    "provider_params": json.loads(a.provider_params) if a.provider_params else None
                }
                for a in assignments
            ]
        except Exception as e:
            logger.error(f"Error reading provider assignments: {e}")
            raise HTTPException(status_code=500, detail=str(e))
    ```

---

## Phase 6: Testing (2 days)

### 6.1 Unit Tests - Utilities

- [x] **Test geo_normalization utilities**
  - File: `backend/test_scripts/test_utilities/test_geo_normalization.py`
  - Already created in Phase 2.2
  - Run: `./test_runner.py utils geo-normalization`
  - **Expected**: âœ… All tests pass (Last run Nov 20, 2025 @09:56)

### 6.2 Unit Tests - Service Layer

- [x] **Test AssetMetadataService**
  - File: `backend/test_scripts/test_services/test_asset_metadata.py` (NEW - 125 lines)
  - Test cases:
    1. âœ… parse_classification_params(): valid JSON â†’ model
    2. âœ… parse_classification_params(): None â†’ None
    3. âœ… serialize_classification_params(): model â†’ JSON
    4. âœ… serialize_classification_params(): None â†’ None
    5. âœ… compute_metadata_diff(): old vs new â†’ changes list
    6. âœ… apply_partial_update(): absent fields ignored
    7. âœ… apply_partial_update(): null clears field
    8. âœ… apply_partial_update(): geographic_area full replace
    9. âœ… apply_partial_update(): invalid geographic_area â†’ raises ValueError
    10. âœ… merge_provider_metadata(): provider data merges correctly
  - Run: `./test_runner.py services asset-metadata`
  - **Status**: âœ… PASS (Nov 20, 2025 @10:34) - All 10 test functions passing
  - **Note**: Fixed Pydantic v2 deprecation warnings (removed json_encoders from fx.py)

- [x] **Test provider metadata auto-populate**
  - File: `backend/test_scripts/test_services/test_asset_source.py` (UPDATED +70 lines)
  - Added test: `test_metadata_auto_populate()` - Test 6a in orchestration
  - Mock: Uses mockprov provider with metadata support
  - Verify: âœ… After assignment, asset.classification_params is populated
  - Verify: âœ… Metadata changes in result dict (4 fields: sector, investment_type, short_description, geographic_area)
  - Verify: âœ… Result includes metadata_updated flag
  - Run: `./test_runner.py services asset-source`
  - **Status**: âœ… PASS (Nov 20, 2025 @10:43) - All 16/16 tests passing including metadata auto-populate
  - **Output**: Metadata auto-populated: {"investment_type":"stock","short_description":"Mock test asset...

### 6.3 Integration Tests - API

- [ ] **Test PATCH metadata endpoint**
  - File: `backend/test_scripts/test_api/test_assets_metadata.py` (NEW)
  - Test cases:
    1. PATCH with valid geographic_area â†’ 200, updated
    2. PATCH with invalid geographic_area â†’ 422, error details
    3. PATCH with absent fields â†’ 200, fields unchanged
    4. PATCH with null â†’ 200, fields cleared
    5. PATCH empty payload â†’ 200, no changes
  - Run: `./test_runner.py api assets-metadata`

- [ ] **Test bulk read assets**
  - File: `backend/test_scripts/test_api/test_assets_metadata.py` (UPDATE)
  - Test: POST /api/v1/assets with asset_ids list
  - Verify: Returns array of AssetMetadataResponse
  - Run: `./test_runner.py api assets-metadata`

- [ ] **Test metadata refresh endpoints**
  - File: `backend/test_scripts/test_api/test_assets_metadata.py` (UPDATE)
  - Test cases:
    1. Single refresh: POST /{asset_id}/metadata/refresh â†’ MetadataRefreshResult
    2. Bulk refresh: POST /metadata/refresh/bulk â†’ partial success
    3. No provider assigned â†’ success=false, appropriate message
    4. Provider doesn't support metadata â†’ success=false
  - Run: `./test_runner.py api assets-metadata`

- [ ] **Test provider assignment bulk read**
  - File: `backend/test_scripts/test_api/test_assets_metadata.py` (UPDATE)
  - Test: POST /api/v1/assets/scraper with asset_ids
  - Verify: Returns provider assignments
  - Run: `./test_runner.py api assets-metadata`

### 6.4 Edge Cases & Error Handling

- [ ] **Test geographic_area edge cases**
  - File: `backend/test_scripts/test_utilities/test_geo_normalization.py` (UPDATE)
  - Cases:
    1. Sum = 0.999999 (within tolerance) â†’ normalized to 1.0
    2. Sum = 1.000001 (within tolerance) â†’ normalized to 1.0
    3. Sum = 0.95 (out of tolerance) â†’ ValueError
    4. Sum = 1.05 (out of tolerance) â†’ ValueError
    5. Single country weight = 1.0 â†’ valid
    6. Empty dict â†’ ValueError (no countries)
    7. Negative weight â†’ ValueError
    8. Zero weight â†’ valid (country with 0% allocation)

- [ ] **Test PATCH semantic edge cases**
  - File: `backend/test_scripts/test_services/test_asset_metadata.py` (UPDATE)
  - Cases:
    1. PATCH with only geographic_area â†’ other fields unchanged
    2. PATCH geographic_area=null â†’ clears existing geographic_area
    3. Multiple PATCHes in sequence â†’ final state correct
    4. Concurrent PATCHes (optimistic locking) â†’ last write wins

---

## Phase 7: Documentation (1 day)

### 7.1 API Documentation

- [ ] **Update OpenAPI/Swagger documentation**
  - Files: Docstrings in `api/v1/assets.py` endpoints
  - Ensure: All new endpoints have comprehensive docstrings
  - Include: Request/response examples with geographic_area

- [ ] **Create API examples document**
  - File: `docs/api-examples/metadata-management.md` (NEW)
  - Sections:
    1. PATCH metadata with geographic_area
    2. Bulk read assets with metadata
    3. Refresh metadata from provider
    4. Read provider assignments
  - Include: cURL examples and expected responses

### 7.2 Developer Documentation

- [ ] **Update database schema documentation**
  - File: `docs/database-schema.md`
  - Add: classification_params field description
  - Add: JSON structure example
  - Add: Validation rules (geographic_area sum=1.0)

- [ ] **Create metadata management guide**
  - File: `docs/metadata-management.md` (NEW)
  - Sections:
    1. Overview (classification_params structure)
    2. Geographic area validation rules
    3. PATCH semantics (absent vs null)
    4. Provider metadata auto-populate
    5. Normalization process (pycountry, Decimal quantization)
    6. Troubleshooting common validation errors

### 7.3 Code Documentation

- [ ] **Add comprehensive docstrings**
  - Files: All new modules/classes/methods
  - Standard: Google-style docstrings
  - Include: Args, Returns, Raises, Examples

- [ ] **Update FEATURE_COVERAGE_REPORT**
  - File: `LibreFolio_developer_journal/FEATURE_COVERAGE_REPORT.md`
  - Add section: Phase 5.1 - Asset Metadata & Classification
  - Include: Features implemented, test coverage, endpoints added

---

## Phase 8: Final Validation & Cleanup (1 day)

### 8.1 End-to-End Testing

- [ ] **Manual E2E test scenario**
  - Start server: `./dev.sh backend`
  - Create asset with yfinance provider
  - Verify metadata auto-populated
  - PATCH metadata with geographic_area
  - Verify changes persisted
  - Bulk read assets â†’ verify metadata returned
  - Refresh metadata â†’ verify updates

- [ ] **Run full test suite**
  - Command: `./test_runner.py all`
  - **Expected**: All tests pass (including new metadata tests)
  - Fix any regressions

### 8.2 Code Quality Checks

- [ ] **Check for inline Pydantic models**
  - Command: `grep -rn "class.*BaseModel" backend/app/api/v1/*.py`
  - **Expected**: 0 results (all schemas in schemas/)

- [ ] **Check import cycles**
  - Test: `python -c "from backend.app.api.v1.assets import router"`
  - **Expected**: No import errors

- [ ] **Check unused imports**
  - Tool: PyCharm inspections or `pylint`
  - **Action**: Remove any unused imports

### 8.3 Performance Validation

- [ ] **Benchmark bulk operations**
  - Test: Bulk read 100 assets with metadata
  - Test: Bulk metadata refresh 50 assets
  - **Target**: < 5 seconds for bulk operations

- [ ] **Check database query count**
  - Enable SQLAlchemy logging
  - Test: Bulk operations should use â‰¤ 3 queries
  - **Optimization**: Ensure no N+1 query patterns

### 8.4 Documentation Review

- [ ] **Review all new documentation**
  - Check: Spelling, grammar, technical accuracy
  - Check: Examples are executable and correct
  - Check: Links between documents work

- [ ] **Update main README if needed**
  - Add: Mention of metadata management features
  - Add: Link to metadata-management.md guide

---

## Completion Checklist

### Code Completeness
- [ ] All schemas defined in `backend/app/schemas/` (NO inline models)
- [ ] All utilities in `backend/app/utils/`
- [ ] All services in `backend/app/services/`
- [ ] All endpoints in `backend/app/api/v1/`
- [ ] All tests in `backend/test_scripts/`

### Quality Gates
- [ ] All tests passing (utils, services, API)
- [ ] No import cycles
- [ ] No inline Pydantic models in API layer
- [ ] Database migration applied and tested
- [ ] Documentation complete and accurate

### Functional Requirements
- [ ] PATCH metadata with partial update semantics
- [ ] Geographic area validation (ISO-3166-A3, sum=1.0)
- [ ] Provider metadata auto-populate on assignment
- [ ] Bulk operations with partial success support
- [ ] Metadata refresh from provider
- [ ] Bulk read assets and provider assignments

### Non-Functional Requirements
- [ ] Bulk-first API pattern
- [ ] Pydantic v2 with FA prefix
- [ ] Code reuse maximized
- [ ] Performance acceptable (< 5s for bulk ops)
- [ ] Logging comprehensive (structlog)

---

## Success Metrics

**Time Estimate**: 4-6 days (32-48 hours)

**Completion Criteria**:
1. âœ… All 80+ checklist items completed
2. âœ… All tests passing (100% of new tests)
3. âœ… Documentation complete
4. âœ… E2E scenario works end-to-end
5. âœ… Code quality gates passed

**Deliverables**:
- Database schema updated (classification_params column)
- 10+ new Pydantic schemas (reusing existing patterns)
- 3+ utility modules (geo_normalization, metadata service)
- 5+ new API endpoints (PATCH, POST bulk, refresh)
- 50+ test cases (utilities, services, API)
- 5+ documentation files (API examples, guides)

---

**Next Phase**: Phase 5.2 - Advanced Provider Implementations with metadata fetch
